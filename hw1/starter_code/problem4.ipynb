{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You will implement value iteration and policy iteration for the Frozen Lake environment from OpenAI Gym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import needed libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from lake_envs import *\n",
    "from utils import render_single\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ! Attention: read the following to get familiar with the parameters of the functions that you will implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor policy_evaluation, policy_improvement, policy_iteration and value_iteration,\\nthe parameters P, nS, nA, gamma are defined as follows:\\n\\n\\tP: nested dictionary\\n\\t\\tFrom gym.core.Environment\\n\\t\\tFor each pair of states in [0, nS-1] and actions in [0, nA-1], P[state][action] is \\n        (a list of) tuples of the form (probability, nextstate, reward, terminal) where\\n\\t\\t\\t- probability: float\\n\\t\\t\\t\\tthe probability of transitioning from \"state\" to \"nextstate\" with \"action\"\\n\\t\\t\\t- nextstate: int\\n\\t\\t\\t\\tdenotes the state we transition to (in range [0, nS - 1])\\n\\t\\t\\t- reward: int\\n\\t\\t\\t\\teither 0 or 1, the reward for transitioning from \"state\" to\\n\\t\\t\\t\\t\"nextstate\" with \"action\"\\n\\t\\t\\t- terminal: bool\\n\\t\\t\\t  True when \"nextstate\" is a terminal state (hole or goal), False otherwise\\n\\tnS: int\\n\\t\\tnumber of states in the environment\\n\\tnA: int\\n\\t\\tnumber of actions in the environment\\n\\tgamma: float\\n\\t\\tDiscount factor. Number in range [0, 1)\\n'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "For policy_evaluation, policy_improvement, policy_iteration and value_iteration,\n",
    "the parameters P, nS, nA, gamma are defined as follows:\n",
    "\n",
    "\tP: nested dictionary\n",
    "\t\tFrom gym.core.Environment\n",
    "\t\tFor each pair of states in [0, nS-1] and actions in [0, nA-1], P[state][action] is \n",
    "        (a list of) tuples of the form (probability, nextstate, reward, terminal) where\n",
    "\t\t\t- probability: float\n",
    "\t\t\t\tthe probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n",
    "\t\t\t- nextstate: int\n",
    "\t\t\t\tdenotes the state we transition to (in range [0, nS - 1])\n",
    "\t\t\t- reward: int\n",
    "\t\t\t\teither 0 or 1, the reward for transitioning from \"state\" to\n",
    "\t\t\t\t\"nextstate\" with \"action\"\n",
    "\t\t\t- terminal: bool\n",
    "\t\t\t  True when \"nextstate\" is a terminal state (hole or goal), False otherwise\n",
    "\tnS: int\n",
    "\t\tnumber of states in the environment\n",
    "\tnA: int\n",
    "\t\tnumber of actions in the environment\n",
    "\tgamma: float\n",
    "\t\tDiscount factor. Number in range [0, 1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a) Implement policy iteration [10 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
    "\t\"\"\"Evaluate the value function from a given policy.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tP, nS, nA, gamma:\n",
    "\t\tdefined at beginning of file\n",
    "\tpolicy: np.array[nS]\n",
    "\t\tThe policy to evaluate. Maps states to actions.\n",
    "\ttol: float\n",
    "\t\tTerminate policy evaluation when\n",
    "\t\t\tmax |value_function(s) - prev_value_function(s)| < tol\n",
    "\tReturns\n",
    "\t-------\n",
    "\tvalue_function: np.ndarray[nS]\n",
    "\t\tThe value function of the given policy, where value_function[s] is\n",
    "\t\tthe value of state s\n",
    "\t\"\"\"\n",
    "\n",
    "\tvalue_function = np.zeros(nS)\n",
    "\n",
    "\t############################\n",
    "\tcount = 0\n",
    "\twhile True:\n",
    "\t\tnew_value_function = np.zeros(nS)\n",
    "\n",
    "\t\tfor state in range(nS):\n",
    "\t\t\taction = policy[state]\n",
    "\t\t\tfor probability, nextstate, reward, terminal in P[state][action]:\n",
    "\t\t\t\tnew_value_function[state] += probability * (reward + gamma * value_function[nextstate])\n",
    "\t\t\n",
    "\t\tif np.max(np.abs(new_value_function - value_function)) < tol:\n",
    "\t\t\tprint(\"Policy Evaluation converged in {} iterations.\".format(count))\n",
    "\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\tcount += 1\n",
    "\t\t\tvalue_function = new_value_function\n",
    "\n",
    "\t############################\n",
    "\treturn value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement policy improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "\t\"\"\"Given the value function from policy improve the policy.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tP, nS, nA, gamma:\n",
    "\t\tdefined at beginning of file\n",
    "\tvalue_from_policy: np.ndarray\n",
    "\t\tThe value calculated from the policy\n",
    "\tpolicy: np.array\n",
    "\t\tThe previous policy.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tnew_policy: np.ndarray[nS]\n",
    "\t\tAn array of integers. Each integer is the optimal action to take\n",
    "\t\tin that state according to the environment dynamics and the\n",
    "\t\tgiven value function.\n",
    "\t\"\"\"\n",
    "\n",
    "\tnew_policy = np.zeros(nS, dtype='int')\n",
    "\n",
    "\t############################\n",
    "\t# for state in P:\n",
    "\t# \tfor i, action in enumerate(state):\n",
    "\t# \t\texpectations = []\n",
    "\t# \t\t# sum up probability-weighted sum of values for that state-action pair\n",
    "\t# \t\tfor probability, nextstate, reward, terminal in action:\n",
    "\t# \t\t\texpectations[i] += probability * (reward + value_from_policy[nextstate])\n",
    "\t# \t\t\tnew_policy[state] = np.argmax(expectations)\n",
    "\n",
    "\tfor s in range(nS):\n",
    "\t\t# choose the action that maximizes the expected value\n",
    "\t\tnew_policy[s] = np.argmax([\n",
    "\t\t\tsum(prob * (reward + gamma * value_from_policy[nextstate])\n",
    "\t\t\t\tfor prob, nextstate, reward, terminal in P[s][a])\n",
    "\t\t\tfor a in range(nA)\n",
    "\t\t])\t\t\t\n",
    "\n",
    "\t############################\n",
    "\treturn new_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "\t\"\"\"Runs policy iteration.\n",
    "\n",
    "\tYou should call the policy_evaluation() and policy_improvement() methods to\n",
    "\timplement this method.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tP, nS, nA, gamma:\n",
    "\t\tdefined at beginning of file\n",
    "\ttol: float\n",
    "\t\ttol parameter used in policy_evaluation()\n",
    "\tReturns:\n",
    "\t----------\n",
    "\tvalue_function: np.ndarray[nS]\n",
    "\tpolicy: np.ndarray[nS]\n",
    "\t\"\"\"\n",
    "\n",
    "\tvalue_function = np.zeros(nS)\n",
    "\tpolicy = np.zeros(nS, dtype=int)\n",
    "\n",
    "\t############################\n",
    "\tcount = 0\n",
    "\twhile True:\n",
    "\t\tpolicy = policy_improvement(P, nS, nA, value_function, policy, gamma=0.9)\n",
    "\t\tnew_value_function = policy_evaluation(P, nS, nA, policy, gamma, tol)\n",
    "\t\tif np.max(np.abs(value_function-new_value_function)) < tol:\n",
    "\t\t\tprint(\"Policy Iteration converged in {} iterations.\".format(count))\n",
    "\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\tcount += 1\n",
    "\t\t\tvalue_function = new_value_function\n",
    "\t\t\t\t\t\n",
    "\t############################\n",
    "\treturn value_function, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run policy iteration on 'Deterministic-4x4-FrozenLake-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Evaluation converged in 1 iterations.\n",
      "Policy Evaluation converged in 2 iterations.\n",
      "Policy Evaluation converged in 3 iterations.\n",
      "Policy Evaluation converged in 5 iterations.\n",
      "Policy Evaluation converged in 5 iterations.\n",
      "Policy Evaluation converged in 6 iterations.\n",
      "Policy Evaluation converged in 6 iterations.\n",
      "Policy Iteration converged in 6 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Installs\\Anaconda3\\envs\\drl\\Lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "V_pi, p_pi = policy_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final value function:\n",
      " [[0.59049 0.6561  0.729   0.6561 ]\n",
      " [0.6561  0.      0.81    0.     ]\n",
      " [0.729   0.81    0.9     0.     ]\n",
      " [0.      0.9     1.      0.     ]]\n",
      "\n",
      "Final policy:\n",
      " [[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "# print the final value function and policy\n",
    "print('Final value function:\\n',V_pi.reshape(4,4))\n",
    "print('\\nFinal policy:\\n',p_pi.reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# test the policy in the environment\n",
    "render_single(env, p_pi, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run policy iteration on 'Stochastic-4x4-FrozenLake-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Evaluation converged in 8 iterations.\n",
      "Policy Evaluation converged in 18 iterations.\n",
      "Policy Evaluation converged in 20 iterations.\n",
      "Policy Evaluation converged in 26 iterations.\n",
      "Policy Evaluation converged in 26 iterations.\n",
      "Policy Evaluation converged in 26 iterations.\n",
      "Policy Iteration converged in 5 iterations.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "V_pi, p_pi = policy_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final value function:\n",
      " [[0.06144365 0.05515526 0.06984368 0.05087275]\n",
      " [0.08503352 0.         0.10970645 0.        ]\n",
      " [0.13983835 0.24364732 0.29690307 0.        ]\n",
      " [0.         0.37709623 0.63751037 0.        ]]\n",
      "\n",
      "Final policy:\n",
      " [[0 3 0 3]\n",
      " [0 0 0 0]\n",
      " [3 1 0 0]\n",
      " [0 2 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# print the final value function and policy\n",
    "print('Final value function:\\n',V_pi.reshape(4,4))\n",
    "print('\\nFinal policy:\\n',p_pi.reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# test the policy in the environment\n",
    "render_single(env, p_pi, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b) Implement value iteration [10 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "\t\"\"\"\n",
    "\tLearn value function and policy by using value iteration method for a given\n",
    "\tgamma and environment.\n",
    "\n",
    "\tParameters:\n",
    "\t----------\n",
    "\tP, nS, nA, gamma:\n",
    "\t\tdefined at beginning of file\n",
    "\ttol: float\n",
    "\t\tTerminate value iteration when\n",
    "\t\t\tmax |value_function(s) - prev_value_function(s)| < tol\n",
    "\tReturns:\n",
    "\t----------\n",
    "\tvalue_function: np.ndarray[nS]\n",
    "\tpolicy: np.ndarray[nS]\n",
    "\t\"\"\"\n",
    "\n",
    "\tvalue_function = np.zeros(nS)\n",
    "\tpolicy = np.zeros(nS, dtype=int)\n",
    "\t############################\n",
    "\tnew_value_function = np.zeros(nS)\n",
    "\tcounter = 0\n",
    "\twhile True:\n",
    "\t\tfor s in range(nS):\n",
    "\t\t\tnew_value_function[s] = np.max([\n",
    "\t\t\t\tsum(probability * (reward + gamma * value_function[nextstate])\n",
    "\t\t\t\tfor probability, nextstate, reward, _ in P[s][a]) \n",
    "\t\t\t\tfor a in range(nA)\n",
    "\t\t\t])\n",
    "\t\tif np.max(np.abs(new_value_function - value_function)) < tol:\n",
    "\t\t\tprint(\"Value Iteration converged in {} iterations.\".format(counter))\n",
    "\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\tvalue_function = new_value_function.copy()\n",
    "\t\t\tcounter += 1\n",
    "\tfor s in range(nS):\n",
    "\t\tpolicy[s] = np.argmax([\n",
    "\t\t\tsum(probability*(reward + gamma * value_function[nextstate])\n",
    "\t   \t\tfor probability, nextstate, reward, _ in P[s][a])\n",
    "\t\t\tfor a in range(nA)\n",
    "\t\t])\n",
    "\t############################\n",
    "\treturn value_function, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run value iteration on 'Deterministic-4x4-FrozenLake-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration converged in 6 iterations.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "V_vi, p_vi = value_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final value function:\n",
      " [[0.59049 0.6561  0.729   0.6561 ]\n",
      " [0.6561  0.      0.81    0.     ]\n",
      " [0.729   0.81    0.9     0.     ]\n",
      " [0.      0.9     1.      0.     ]]\n",
      "\n",
      "Final policy:\n",
      " [[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "# print the final value function and policy\n",
    "print('Final value function:\\n',V_vi.reshape(4,4))\n",
    "print('\\nFinal policy:\\n',p_vi.reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# test the policy in the environment\n",
    "render_single(env, p_vi, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run value iteration on 'Stochastic-4x4-FrozenLake-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration converged in 26 iterations.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "V_vi, p_vi = value_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final value function:\n",
      " [[0.06162274 0.05531399 0.06996222 0.05101703]\n",
      " [0.08519461 0.         0.10976852 0.        ]\n",
      " [0.13996615 0.2437311  0.29696299 0.        ]\n",
      " [0.         0.37715398 0.63753958 0.        ]]\n",
      "\n",
      "Final policy:\n",
      " [[0 3 0 3]\n",
      " [0 0 0 0]\n",
      " [3 1 0 0]\n",
      " [0 2 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# print the final value function and policy\n",
    "print('Final value function:\\n',V_vi.reshape(4,4))\n",
    "print('\\nFinal policy:\\n',p_vi.reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# test the policy in the environment\n",
    "render_single(env, p_vi, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c) Answer questions [5 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have run both methods on the Deterministic-4x4-FrozenLake-v0 and Stochastic-4x4-FrozenLake-v0 environments. In the second environment, the dynamics of the world are stochastic. How does stochasticity affect the number of iterations required, and the resulting policy? Write your answer below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policies in stochastic environments prioritize avoiding holes over reaching the reward quickly. Hence, it takes longer to converge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
